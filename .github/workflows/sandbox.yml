name: Sandbox

on: [workflow_dispatch, push, pull_request]

jobs:

  canary-multi-turn:
    runs-on: ubuntu-22.04
    timeout-minutes: 10
    strategy:
      fail-fast: false
      matrix:
        model:
          - meta-llama/llama-3.2-3b-instruct             # $0.0200/$0.0200 [ 128K]
    steps:
      - uses: actions/checkout@v4

      - run: ./chat-llm.js tests/en/canary-single-turn.txt
        env:
          LLM_API_BASE_URL: https://openrouter.ai/api/v1
          LLM_API_KEY: ${{ secrets.LLM_API_KEY }}
          LLM_CHAT_MODEL: ${{ matrix.model }}

      - run: ./chat-llm.js tests/en/canary-multi-turn.txt
        env:
          LLM_API_BASE_URL: https://openrouter.ai/api/v1
          LLM_API_KEY: ${{ secrets.LLM_API_KEY }}
          LLM_CHAT_MODEL: ${{ matrix.model }}

  high-school-stem:
    runs-on: ubuntu-22.04
    timeout-minutes: 10
    strategy:
      fail-fast: false
      matrix:
        model:
          - meta-llama/llama-3.2-3b-instruct             # $0.0200/$0.0200 [  16K]
    steps:
      - uses: actions/checkout@v4
      - run: ./chat-llm.js tests/en/high-school-stem.txt
        env:
          LLM_API_BASE_URL: https://openrouter.ai/api/v1
          LLM_API_KEY: ${{ secrets.LLM_API_KEY }}
          LLM_CHAT_MODEL: ${{ matrix.model }}

  general-knowledge:
    runs-on: ubuntu-22.04
    timeout-minutes: 10
    strategy:
      fail-fast: false
      matrix:
        model:
          - meta-llama/llama-3.2-3b-instruct             # $0.0200/$0.0200 [  16K]
    steps:
      - uses: actions/checkout@v4
      - run: ./chat-llm.js tests/en/general-knowledge.txt
        env:
          LLM_API_BASE_URL: https://openrouter.ai/api/v1
          LLM_API_KEY: ${{ secrets.LLM_API_KEY }}
          LLM_CHAT_MODEL: ${{ matrix.model }}