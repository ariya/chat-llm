#!/usr/bin/env -S sh -c 'command -v node >/dev/null && exec node "$0" "$@" || exec bun "$0" "$@" '

const fs = require('fs');
const http = require('http');
const readline = require('readline');
const { analyzeSentiment } = require('./tools/sentiment_analyzer');

const LLM_API_BASE_URL = process.env.LLM_API_BASE_URL || 'https://api.openai.com/v1';
const LLM_API_KEY = process.env.LLM_API_KEY || process.env.OPENAI_API_KEY;
const LLM_CHAT_MODEL = process.env.LLM_CHAT_MODEL;
const LLM_STREAMING = process.env.LLM_STREAMING !== 'no';
const LLM_FORCE_REASONING = process.env.LLM_FORCE_REASONING;

const LLM_DEBUG = process.env.LLM_DEBUG;
const LLM_DEBUG_FAIL_EXIT = process.env.LLM_DEBUG_FAIL_EXIT;

const NORMAL = '\x1b[0m';
const BOLD = '\x1b[1m';
const YELLOW = '\x1b[93m';
const MAGENTA = '\x1b[35m';
const RED = '\x1b[91m';
const GREEN = '\x1b[92m';
const CYAN = '\x1b[36m';
const GRAY = '\x1b[90m';
const ARROW = '⇢';
const CHECK = '✓';
const CROSS = '✘';

/**
 * Suspends the execution for a specified amount of time.
 *
 * @param {number} ms - The amount of time to suspend execution in milliseconds.
 * @returns {Promise<void>} - A promise that resolves after the specified time has elapsed.
 */
const sleep = async (ms) => new Promise((resolve) => setTimeout(resolve, ms));

const MAX_RETRY_ATTEMPT = 3;


/**
 * Represents a chat message.
 *
 * @typedef {Object} Message
 * @property {'system'|'user'|'assistant'} role
 * @property {string} content
 */

/**
 * A callback function to stream then completion.
 *
 * @callback CompletionHandler
 * @param {string} text
 * @returns {void}
 */

/**
 * Generates a chat completion using a RESTful LLM API service.
 *
 * @param {Array<Message>} messages - List of chat messages.
 * @param {CompletionHandler=} handler - An optional callback to stream the completion.
 * @returns {Promise<string>} The completion generated by the LLM.
 */

const chat = async (messages, handler = null, attempt = MAX_RETRY_ATTEMPT) => {
    const timeout = 17; // seconds
    const stream = LLM_STREAMING && typeof handler === 'function';
    const model = LLM_CHAT_MODEL || 'gpt-5-nano';
    const url = `${LLM_API_BASE_URL}/chat/completions`
    const auth = (LLM_API_KEY) ? { 'Authorization': `Bearer ${LLM_API_KEY}` } : {};
    const stop = ['\\boxed', '</s>', '</s>', '</s>'];

    const body = { messages, model, stop, stream }

    LLM_DEBUG &&
        messages.forEach(({ role, content }) => {
            console.log(`${MAGENTA}${role}:${NORMAL} ${content}`);
        });

    try {

        const response = await fetch(url, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json', ...auth },
            body: JSON.stringify(body),
            signal: AbortSignal.timeout(timeout * 1000)
        });
        if (!response.ok) {
            const status = response.status;
            const statusText = response.statusText;
            
            // Handle rate limiting with exponential backoff
            if (status === 429 && attempt > 1) {
                const waitTime = (MAX_RETRY_ATTEMPT - attempt + 2) * 5000; // 5s, 10s, 15s
                LLM_DEBUG && console.log(`Rate limited (429). Waiting ${waitTime}ms before retry...`);
                await sleep(waitTime);
                return await chat(messages, handler, attempt - 1);
            }
            
            throw new Error(`HTTP error with the status: ${status} ${statusText}`);
        }

        const extract = (data) => {
            const { choices, candidates } = data;
            const first = choices ? choices[0] : candidates[0];
            if (first?.content || first?.message) {
                const content = first?.content ? first.content : first.message.content;
                const parts = content?.parts;
                const answer = parts ? parts.map(part => part.text).join('') : content;
                return answer;
            }
            return '';
        }

        if (!stream) {
            const data = await response.json();
            const answer = extract(data).trim();
            if (LLM_DEBUG) {
                console.log(`${YELLOW}${answer}${NORMAL}`);
            }
            (answer.length > 0) && handler && handler(answer);
            return answer;
        }

        const parse = (line) => {
            const separator = line.indexOf(':');
            if (separator < 0) {
                return '';
            }
            const key = line.substring(0, separator).trim();
            const payload = line.substring(separator + 1);
            if (key === 'data') {
                let partial = null;
                try {
                    const { choices } = JSON.parse(payload);
                    const [choice] = choices;
                    const { delta } = choice;
                    partial = delta?.content || '';
                } catch (e) {
                    // ignore
                } finally {
                    return partial;
                }
            } else {
                return '';
            }
        }

        const reader = response.body.getReader();
        const decoder = new TextDecoder();

        let answer = '';
        let buffer = '';
        while (true) {
            const { value, done } = await reader.read();
            if (done) {
                break;
            }
            const lines = decoder.decode(value).split('\n');
            for (let i = 0; i < lines.length; ++i) {
                const line = buffer + lines[i];
                if (line[0] === ':') {
                    buffer = '';
                    continue;
                }
                if (line === 'data: [DONE]') {
                    break;
                }
                if (line.length > 0) {
                    const partial = parse(line.trim());
                    if (partial === null) {
                        buffer = line;
                    } else if (partial && partial.length > 0) {
                        buffer = '';
                        if (answer.length < 1) {
                            const leading = partial.trim();
                            answer = leading;
                            handler && (leading.length > 0) && handler(leading);
                        } else {
                            answer += partial;
                            handler && handler(partial);
                        }
                    }
                }
            }
        }
        return answer;
    } catch (e) {
        if (e.name === 'TimeoutError') {
            LLM_DEBUG && console.log(`Timeout with LLM chat after ${timeout} seconds`);
        }
        if (attempt > 1 && (e.name === 'TimeoutError' || e.name === 'EvalError')) {
            LLM_DEBUG && console.log('Retrying...');
            await sleep((MAX_RETRY_ATTEMPT - attempt + 1) * 1500);
            return await chat(messages, handler, attempt - 1);
        } else {
            throw e;
        }
    }
}

const REPLY_PROMPT = `You are a helpful AI assistant. You are chatting with a human user.
Your answer should be a sentence or two, unless the user's request requires long-form outputs.
Never use emojis. Never use markdown. Always answer in plain text.`;

const REPLY_THINK = `You are a helpful AI assistant. You are chatting with a human user.

You have access to the following tools:
- analyzeSentiment(text: string): Analyzes the sentiment of the given text. Returns an object with 'sentiment' (positive, negative, neutral) and 'score'.

You should first draft your thinking process (inner monologue) until you have derived the final answer.
Write both your thoughts and answer in the same language as the task posed by the user.

Your thinking process must follow the template below:

<think>
Your thoughts or/and draft, like working through an exercise on scratch paper.
Be as casual and as long as you want until you are confident to generate a correct answer.
If you need to use a tool, use the following format:
<tool_code>
console.log(analyzeSentiment("text to analyze"));
</tool_code>
</think>

Your answer should be a sentence or two, unless the user's request requires long-form outputs.
Never use emojis. Never use markdown. Always answer in plain text and  in the same language as the query.`;

const DEMO_RESPONSES = {
    'capital': ['Paris is the capital of France.', 'The capital of France is Paris.'],
    'weather': ['I cannot check the current weather, but you can check a weather service online.', 'Weather information is not available in demo mode.'],
    'time': ['I cannot provide real-time information in demo mode.', 'Time-based queries are not supported in demo mode.'],
    'default': ['This is a demo response since the LLM API is unavailable.', 'I am in demo mode and cannot provide real responses.', 'Demo mode is active - API responses are simulated.']
};

const demoReply = async (inquiry) => {
    // Simple pattern matching for demo responses
    const lowerInquiry = inquiry.toLowerCase();
    
    if (lowerInquiry.includes('capital') || lowerInquiry.includes('france') || lowerInquiry.includes('paris')) {
        return DEMO_RESPONSES.capital[Math.floor(Math.random() * DEMO_RESPONSES.capital.length)];
    } else if (lowerInquiry.includes('weather') || lowerInquiry.includes('rain') || lowerInquiry.includes('temperature')) {
        return DEMO_RESPONSES.weather[Math.floor(Math.random() * DEMO_RESPONSES.weather.length)];
    } else if (lowerInquiry.includes('time') || lowerInquiry.includes('hour') || lowerInquiry.includes('clock')) {
        return DEMO_RESPONSES.time[Math.floor(Math.random() * DEMO_RESPONSES.time.length)];
    }
    
    return DEMO_RESPONSES.default[Math.floor(Math.random() * DEMO_RESPONSES.default.length)];
};

const reply = async (context) => {
    const { inquiry, history, delegates } = context;
    const { stream } = delegates || {};

    const messages = [];
    messages.push({ role: 'system', content: LLM_FORCE_REASONING ? REPLY_THINK : REPLY_PROMPT });
    const relevant = history.slice(-5);
    relevant.forEach(msg => {
        const { inquiry, answer } = msg;
        messages.push({ role: 'user', content: inquiry });
        messages.push({ role: 'assistant', content: answer });
    });

    messages.push({ role: 'user', content: inquiry });
    
    let rawAnswer;
    try {
        rawAnswer = await chat(messages, stream);
    } catch (error) {
        if (process.env.LLM_DEMO_MODE) {
            rawAnswer = await demoReply(inquiry);
        } else {
            throw error;
        }
    }

    const THINK_START_TAG = '<think>';
    const THINK_STOP_TAG = '</think>';
    const TOOL_CODE_START_TAG = '<tool_code>';
    const TOOL_CODE_STOP_TAG = '</tool_code>';

    const thinkStartIndex = rawAnswer.indexOf(THINK_START_TAG);
    const thinkStopIndex = rawAnswer.indexOf(THINK_STOP_TAG);


    if (thinkStartIndex !== -1 && thinkStopIndex !== -1) {
        const thinkContent = rawAnswer.substring(thinkStartIndex + THINK_START_TAG.length, thinkStopIndex);
        const toolCodeStartIndex = thinkContent.indexOf(TOOL_CODE_START_TAG);
        const toolCodeStopIndex = thinkContent.indexOf(TOOL_CODE_STOP_TAG);

        if (toolCodeStartIndex !== -1 && toolCodeStopIndex !== -1) {
            const toolCode = thinkContent.substring(toolCodeStartIndex + TOOL_CODE_START_TAG.length, toolCodeStopIndex);
            let toolOutput = '';
            try {
                // Execute the tool code in a sandboxed environment
                // For simplicity, using eval here. In a real-world scenario, consider a more secure sandbox.
                const consoleLog = (output) => { toolOutput += JSON.stringify(output) + '\n'; };
                const context = { analyzeSentiment, console: { log: consoleLog } };
                const script = new Function('analyzeSentiment', 'console', toolCode);
                script(context.analyzeSentiment, context.console);
            } catch (e) {
                toolOutput = `Error executing tool: ${e.message}`;
            }

            // Re-prompt the LLM with the tool output
            messages.push({ role: 'assistant', content: rawAnswer }); // Add the LLM's initial thought process
            messages.push({ role: 'system', content: `Tool output:\n${toolOutput}` });
            messages.push({ role: 'user', content: 'Given the tool output, provide your final answer.' });
            rawAnswer = await chat(messages, stream);
        }
    }

    return { answer: rawAnswer, ...context };
}

/**
 * Converts an expected answer into a suitable regular expression array.
 *
 * @param {string} match
 * @returns {Array<RegExp>}
 */
const regexify = (match) => {
    const filler = (text, index) => {
        let i = index;
        while (i < text.length) {
            if (text[i] === '/') {
                break;
            }
            ++i;
        }
        return i;
    };

    const pattern = (text, index) => {
        let i = index;
        if (text[i] === '/') {
            ++i;
            while (i < text.length) {
                if (text[i] === '/' && text[i - 1] !== '\\') {
                    break;
                }
                ++i;
            }
        }
        return i;
    };

    const regexes = [];
    let pos = 0;
    while (pos < match.length) {
        pos = filler(match, pos);
        const next = pattern(match, pos);
        if (next > pos && next < match.length) {
            const sub = match.substring(pos + 1, next);
            const regex = RegExp(sub, 'gi');
            regexes.push(regex);
            pos = next + 1;
        } else {
            break;
        }
    }

    if (regexes.length === 0) {
        regexes.push(RegExp(match, 'gi'));
    }

    return regexes;
}

/**
 * Returns all possible matches given a list of regular expressions.
 *
 * @param {string} text
 * @param {Array<RegExp>} regexes
 * @returns {Array<Span>}
 */
const match = (text, regexes) => {
    return regexes.map(regex => {
        const match = regex.exec(text);
        if (!match) {
            return null;
        }
        const [first] = match;
        const { index } = match;
        const { length } = first;
        return { index, length };
    }).filter(span => span !== null);
}

/**
 * Formats the input (using ANSI colors) to highlight the spans.
 *
 * @param {string} text
 * @param {Array<Span>} spans
 * @param {string} color
 * @returns {string}
 */

const highlight = (text, spans, color = BOLD + GREEN) => {
    let result = text;
    spans.sort((p, q) => q.index - p.index).forEach((span) => {
        const { index, length } = span;
        const prefix = result.substring(0, index);
        const content = result.substring(index, index + length);
        const suffix = result.substring(index + length);
        result = `${prefix}${color}${content}${NORMAL}${suffix}`;
    });
    return result;
}

/**
 * Evaluates a test file and executes the test cases.
 *
 * @param {string} filename - The path to the test file.
 */
const evaluate = async (filename) => {
    try {
        let history = [];
        let total = 0;
        let failures = 0;

        const handle = async (line) => {
            const parts = (line && line.length > 0) ? line.split(':') : [];
            if (parts.length >= 2) {
                const role = parts[0];
                const content = line.slice(role.length + 1).trim();
                if (role === 'Story') {
                    console.log();
                    console.log('-----------------------------------');
                    console.log(`Story: ${MAGENTA}${BOLD}${content}${NORMAL}`);
                    console.log('-----------------------------------');
                    history = [];
                } else if (role === 'User') {
                    const inquiry = content;
                    const context = { inquiry, history };
                    process.stdout.write(`  ${inquiry}\r`);
                    const start = Date.now();
                    const result = await reply(context);
                    const duration = Date.now() - start;
                    const { answer } = result;
                    history.push({ inquiry, answer: unthink(answer).trim(), duration });
                    ++total;
                } else if (role === 'Assistant') {
                    const expected = content;
                    const last = history.slice(-1).pop();
                    if (!last) {
                        console.error('There is no answer yet!');
                        process.exit(-1);
                    } else {
                        const { inquiry, answer, duration } = last;
                        const target = answer;
                        const regexes = regexify(expected);
                        const matches = match(target, regexes);
                        if (matches.length === regexes.length) {
                            console.log(`${GREEN}${CHECK} ${CYAN}${inquiry} ${GRAY}[${duration} ms]${NORMAL}`);
                            console.log(' ', highlight(target, matches));
                        } else {
                            ++failures;
                            console.error(`${RED}${CROSS} ${YELLOW}${inquiry} ${GRAY}[${duration} ms]${NORMAL}`);
                            console.error(`Expected ${role} to contain: ${CYAN}${regexes.join(',')}${NORMAL}`);
                            console.error(`Actual ${role}: ${MAGENTA}${target}${NORMAL}`);
                            LLM_DEBUG_FAIL_EXIT && process.exit(-1);
                        }
                    }
                }
            }
        };

        const trim = (input) => {
            const text = input.trim();
            const marker = text.indexOf('#');
            if (marker >= 0) {
                return text.substr(0, marker).trim();
            }
            return text;
        }

        const lines = fs.readFileSync(filename, 'utf-8').split('\n').map(trim);
        for (const i in lines) {
            await handle(lines[i]);
        }
        if (failures <= 0) {
            console.log(`${GREEN}${CHECK}${NORMAL} SUCCESS: ${GREEN}${total} test(s)${NORMAL}.`);
        } else {
            console.log(`${RED}${CROSS}${NORMAL} FAIL: ${GRAY}${total} test(s), ${RED}${failures} failure(s)${NORMAL}.`);
            process.exit(-1);
        }
    } catch (e) {
        console.error('ERROR:', e.toString());
        process.exit(-1);
    }
}

const THINK_START = '<think>';
const THINK_STOP = '</think>';

const unthink = (input) => {
    const start = input.indexOf(THINK_START);
    if (start < 0) {
        return input;
    }
    const end = input.indexOf(THINK_STOP);
    if (end < 0) {
        return input.substring(0, start);
    }
    return input.substring(0, start) + input.substring(end + 8);
};

const push = (display, input, threshold = THINK_START.length) => {
    let { buffer, written, print } = display;
    buffer += input;
    if (buffer.length < threshold) {
        return { buffer, written: '', print };
    }
    const incoming = unthink(buffer).trim();
    if (incoming.length > written.length) {
        const delta = incoming.substring(written.length);
        print && print(delta);
        written = incoming;
    }
    return { buffer, written, print };
};

const flush = (display) => push(display, '', 0);

/**
 * Represents the contextual information for each pipeline stage.
 *
 * @typedef {Object} Context
 * @property {Array<object>} history
 * @property {string} inquiry
 * @property {string} answer
 * @property {Object.<string, function>} delegates - Impure functions to access the outside world.
 */

const interact = async () => {
    const history = [];

    let loop = true;
    const io = readline.createInterface({ input: process.stdin, output: process.stdout });
    io.on('close', () => { loop = false; });

    const qa = () => {
        io.question(`${YELLOW}>> ${CYAN}`, async (inquiry) => {
            process.stdout.write(NORMAL);
            const print = (text) => process.stdout.write(text);
            let display = { buffer: '', written: '', print };
            const stream = (text) => display = push(display, text);
            const delegates = { stream };
            const context = { inquiry, history, delegates };
            const start = Date.now();
            await reply(context);
            const duration = Date.now() - start;
            display = flush(display);
            const answer = display.written;
            history.push({ inquiry, answer, duration });
            console.log();
            console.log();
            loop && qa();
        })
    }

    qa();
}

/**
 * Starts an HTTP server that listens on the specified port and serves requests.
 *
 * @param {number} port - The port number to listen on.
 */
const serve = async (port) => {
    let history = [];

    const decode = (url) => {
        const parsedUrl = new URL(`http://localhost/${url}`);
        const { search } = parsedUrl;
        return decodeURIComponent(search.substring(1)).trim();
    };

    const server = http.createServer(async (request, response) => {
        const { url } = request;
        if (url === '/health') {
            response.writeHead(200).end('OK');
        } else if (url === '/' || url === '/index.html') {
            response.writeHead(200, { 'Content-Type': 'text/html' });
            response.end(fs.readFileSync('./index.html'));
        } else if (url.startsWith('/chat')) {
            const inquiry = decode(url);
            if (inquiry === '/reset') {
                history = [];
                response.write('History cleared.');
                response.end();
            } else if (inquiry.length > 0) {
                console.log(`${YELLOW}>> ${CYAN}${inquiry}${NORMAL}`);
                response.writeHead(200, { 'Content-Type': 'text/plain' });

                const stream = (text) => {
                    process.stdout.write(text);
                    response.write(text);
                }
                const delegates = { stream };
                const context = { inquiry, history, delegates };
                const start = Date.now();
                const { answer } = await reply(context);
                console.log();
                response.end();
                const duration = Date.now() - start;
                history.push({ inquiry, answer, duration });
            } else {
                response.writeHead(400).end();
            }
        } else {
            console.error(`${url} is 404!`);
            response.writeHead(404);
            response.end();
        }
    });
    server.listen(port);
    console.log('Listening on port', port);
};

const canary = async () => {
    console.log(`Using LLM at ${YELLOW}${LLM_API_BASE_URL}${NORMAL} (model: ${GREEN}${LLM_CHAT_MODEL || 'default'}${NORMAL}).`);
    process.stdout.write(`${ARROW} Checking LLM...\r`);

    const inquiry = 'What is the capital of France?';
    const history = [];
    const context = { inquiry, history };
    try {
        const { answer } = await reply(context);
        LLM_REASONING_ABILITY = answer.includes(THINK_START) && answer.includes(THINK_STOP);
        console.log(`LLM is ${GREEN}ready${NORMAL} (working as expected).`);
        if (LLM_REASONING_ABILITY) {
            console.log(`This is a ${YELLOW}reasoning${NORMAL} model.`);
        } else {
            console.log(`This is a regular model, ${MAGENTA}not${NORMAL} capable of self-reasoning.`);
        }
        console.log();
    } catch (error) {
        if (process.env.LLM_DEMO_MODE) {
            console.log(`${YELLOW}⚠${NORMAL}  LLM unavailable - running in ${CYAN}demo mode${NORMAL}.`);
            console.log('   (Responses will be simulated for testing purposes)\n');
            return;
        }
        console.error(`${CROSS} ${RED}Fatal error: LLM is not ready!${NORMAL}`);
        console.error(error);
        process.exit(-1);
    }
};

(async () => {
    const args = process.argv.slice(2);
    if (args[0] === 'sentiment' && args.length > 1) {
        const textToAnalyze = args.slice(1).join(' ');
        const result = analyzeSentiment(textToAnalyze);
        console.log(JSON.stringify(result, null, 2));
    } else {
        await canary(); // Only run canary if not directly testing sentiment
        if (args.length > 0) {
            args.forEach(evaluate);
        } else {
            const port = parseInt(process.env.HTTP_PORT, 10);
            if (!Number.isNaN(port) && port > 0 && port < 65536) {
                await serve(port);
            } else {
                await interact();
            }
        }
    }
})();
